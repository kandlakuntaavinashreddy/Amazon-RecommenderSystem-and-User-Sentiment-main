{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RecommenderSystem.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WBiNaTHOB2ai",
        "Ykh8iZDpFtbv",
        "8QEYI7ETGxD2",
        "t0WpCoBmHzDI",
        "UnvkWYk8JnE6",
        "qfFOVWGKJyHi",
        "OjpmaMR0sX0K",
        "G55t2r5ardNQ",
        "LjCXZrlvoeR0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Amazon Recommendation system with User Sentiment understanding** # \n",
        "by\n",
        "\n",
        "*Dixitha Kasturi*\n",
        "\n",
        "*Bhargav Konakanchi*\n"
      ],
      "metadata": {
        "id": "xyjhFVXLWY6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The aim of the project is to build a recommendation system( using the user \n",
        "ratings) and also perform sentiment analysis , to understand the overall User sentiment( negative/positive). A total of over 278,677 ***Clothing,shoes and Jewelry*** reviews were analyzed from the 'Amazon Reviews' dataset, which had other categories as well.\n",
        "\n",
        "* Platforms/Sources:\n",
        "  - Google colab for code exection\n",
        "  - <a href=\"http://jmcauley.ucsd.edu/data/amazon/\" target=\"_blank\">Dataset Link</a>\n",
        "\n",
        "* Algorithms : \n",
        "  - Recommendation System - ALS(Alternating least squares)\n",
        "  - Sentiment Analysis - Logistic Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "Al3VOVhiY4tF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBiNaTHOB2ai"
      },
      "source": [
        "## **Mounting G-drive, installing PySpark** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTq85mzHrSM0"
      },
      "source": [
        "!pip install PyDrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tec8J95seQvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmQpytGIPO_D"
      },
      "source": [
        "\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoNLdXCtNcMP"
      },
      "source": [
        "!tar -xvzf spark-3.2.0-bin-hadoop3.2.tgz &> /dev/null\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_li72aWKBpsr"
      },
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEeARXkqDpBP"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhsMum2-SZ-E"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wQNFJzOYWX2p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFVQ9m9VCvHV"
      },
      "source": [
        "## **Sections** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading some basic libraries"
      ],
      "metadata": {
        "id": "3Wlc_TehOyBP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEkNS4O5syDt"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas import DataFrame \n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import functions as fn\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC03uzY_C14J"
      },
      "source": [
        "## **1. Amazon JSON data description and loading** ##\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The Amazon reviews dataset contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014.\n",
        "This dataset includes reviews (ratings, text, helpfulness votes, review time and so on). The file is in JSON format.\n",
        "\n",
        "* A subcategory of 'Clothing, Shoes and Jewelry' is chosen. It has 39387 unique users gave reviews to 23033 distinct products.\n",
        "\n",
        "* Overall there are 278,677 reviews and 9 attributes\n",
        "\n",
        "* Throughout the analysis, other columns were generated and added as required"
      ],
      "metadata": {
        "id": "YrUl5h1KO5zT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format of the reviews: \n",
        "\n",
        "{\n",
        "  \n",
        "  \"reviewerID\": \"A2SUAM1J3GNN3B\",\n",
        "\n",
        "  \"asin\": \"0000013714\",\n",
        "\n",
        "  \"reviewerName\": \"J. McDonald\",\n",
        "\n",
        "  \"helpful\": [2, 3],\n",
        "\n",
        "  \"reviewText\": \"I bought this for my husband who plays the piano.  He is having a wonderful time playing these old hymns.  The music  is at times hard to read because we think the book was published for singing from more than playing from.  Great purchase though!\",\n",
        "\n",
        "  \"overall\": 5.0,\n",
        "\n",
        "  \"summary\": \"Heavenly Highway Hymns\",\n",
        "\n",
        "  \"unixReviewTime\": 1252800000,\n",
        "\n",
        "  \"reviewTime\": \"09 13, 2009\"\n",
        "\n",
        "}\n",
        "\n",
        "where\n",
        "\n",
        "reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
        "\n",
        "asin - ID of the product, e.g. 0000013714\n",
        "\n",
        "reviewerName - name of the reviewer\n",
        "\n",
        "helpful - helpfulness rating of the review, e.g. 2/3\n",
        "\n",
        "reviewText - text of the review\n",
        "\n",
        "overall - rating of the product\n",
        "\n",
        "summary - summary of the review\n",
        "\n",
        "unixReviewTime - time of the review (unix time)\n",
        "\n",
        "reviewTime - time of the review (raw)\n",
        "\n"
      ],
      "metadata": {
        "id": "90nE5NPZXJhI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoUPBzk8C2EK"
      },
      "source": [
        "\n",
        "amazon_spark_df = spark.read.json(\"/content/drive/MyDrive/Clothing_Shoes_and_Jewelry_5.json\")\n",
        "amazon_spark_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7cj_Bs5DO_r"
      },
      "source": [
        "## **2. Exploratory Data Analysis** ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU0L40-Hs4xG"
      },
      "source": [
        "amazon_spark_df.describe('overall').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1Mtdjph9z5b"
      },
      "source": [
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "df1 = amazon_spark_df.select('asin', 'overall', 'reviewText', 'reviewTime',   'reviewerID', 'reviewerName', 'summary', 'unixReviewTime')\n",
        "df1.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df1.columns]).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVMI09a6_hm_"
      },
      "source": [
        "\n",
        "*   The Data overall is clean. Reviewer name has 464 NAN values, but we donot use the reviewer name a lot only for reference purpose, so we keep the values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3zkPgIVA0qp"
      },
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "amazon_spark_df.select(countDistinct('reviewerID')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFMSYpevBOvU"
      },
      "source": [
        "\n",
        "\n",
        "*   39387 unique users gave reviews to products in the shoes, clothing, jewelery category.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAfupIopA8Em"
      },
      "source": [
        "amazon_spark_df.select(countDistinct('asin')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYXS7KFABCzC"
      },
      "source": [
        "\n",
        "\n",
        "*   There are 23033 distinct products in clothing,shoes and jewelery category\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO1LX4GVHJz8"
      },
      "source": [
        "# Getting count of reviews for products\n",
        "group_by_product = amazon_spark_df.groupBy('asin').count().orderBy('count', ascending=False)\n",
        "group_by_product.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usXB1Ln1HNxb"
      },
      "source": [
        "#  Getting count of reviews given by user\n",
        "group_by_user = amazon_spark_df.groupBy('reviewerID','reviewerName').count().orderBy('count', ascending=False)\n",
        "group_by_user.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now check the overall distribution of the ratings. They were distributed from 1 to 5"
      ],
      "metadata": {
        "id": "UFtupc9nf0EA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHwd_KZLDb5h"
      },
      "source": [
        "ratings_row = amazon_spark_df.select('overall').collect()\n",
        "ratings = [ratings_row[i][0] for i in range(len(ratings_row))]\n",
        "plt.hist(ratings,alpha = 1, edgecolor = 'black',histtype='stepfilled', bins = [x * 0.1 for x in range(0,55,5)])\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Rating')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipcHrxAesWl9"
      },
      "source": [
        "\n",
        "\n",
        "*   No 0 ratings\n",
        "*   less low ratings/ Most of the ratings are between 4 to 5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykh8iZDpFtbv"
      },
      "source": [
        "### Creating proper date column from the unixtimestamp ###\n",
        "##### ('Time')  ######\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQsYNWMywrz-"
      },
      "source": [
        "\n",
        "amazon_spark_df.withColumn('reviewTime',fn.trim(fn.col('reviewTime')))\n",
        "amazon_spark_df.withColumn('reviewTime',fn.ltrim(fn.col('reviewTime')))\n",
        "amazon_spark_df.withColumn('reviewTime',fn.rtrim(fn.col('reviewTime')))\n",
        "\n",
        "amazon_spark_df= amazon_spark_df.withColumn( 'Time', fn.from_unixtime(fn.col(\"unixReviewTime\"),\"MM-dd-yyyy\"))\n",
        "amazon_spark_df.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvOPQciAFEmF"
      },
      "source": [
        "### Creating 2 columns : Average rating for each product and by each user ###\n",
        "#####('Average_rating_by_product', 'Average_rating_by_user')#####"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YsUtl-gfR0b"
      },
      "source": [
        "group_by_product_avg = amazon_spark_df.groupBy('asin').agg(fn.mean('overall').alias(\"Average_rating_by_product\"))\n",
        "group_by_user_avg = amazon_spark_df.groupBy('reviewerID').agg(fn.mean('overall').alias(\"Average_rating_by_user\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU0LJjJFl3Jt"
      },
      "source": [
        "merged_df = amazon_spark_df.join(group_by_product_avg,['asin'],how='full')\n",
        "amazon_spark_df = merged_df.join(group_by_user_avg,['reviewerID'],how='full')\n",
        "amazon_spark_df.count()\n",
        "amazon_spark_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUN3mlqEU1jF"
      },
      "source": [
        "amazon_spark_df.select('overall','asin','Average_rating_by_product','reviewerID','Average_rating_by_user','unixReviewTime','Time').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJp1KdxgH-Gv"
      },
      "source": [
        "## **3. Collaborative filtering Recommendation System Model : ALS** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview of Recommendation System : \n",
        "\n",
        "From all of the available methods/techniques, Collaborative Filtering was used. g. It's called collaborative because it makes recommendations based on other people in effect, people collaborate( the algorithm does this) to come up with recommendations. This method aim to fill in the missing entries of a user-item association matrix. We will only be considering users and what items a user has interacted with( here interaction means which products the user has given a review/rating for). In real world, clicks/views besides what is bought previously and what ratings are given, are all used. \n",
        "\n",
        "We are dealing with Explicit data(ratings) instead of implicit(views).For instance according to our data, with ratings we know that a 1 means the user did not like that item and a 5 that he/she really liked it. Using our interaction term(ratings) from other users and the considered user, we generate recommendations of products which he/she might like.\n",
        "\n",
        "Approach used: \n",
        "1. Checked the sparsity of user-item matrix\n",
        "2. Converting all columns to Numeric for ALS\n",
        "3. 70:30 Training and Testing split\n",
        "4. ALS model generation with parameter tuning\n",
        "5. Evaluating RMSE\n",
        "6. Generating Recommendations\n",
        "\n",
        "Under collaborative filtering, the one that is supported by spark is Matrix Factorization method known as ALS(Alternating least squares).\n",
        "\n",
        "> Alternating least Squares \n",
        "\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1VaKRCiDf9fwfDOLmqfJujV6C-nbLTJzV)\n",
        "\n",
        "\n",
        "The user-item utility matrix R where the values denotes how item i has been rated by user u on a scale of 1â€“5. It is a sparse matrix. The goal is to generate values that are missing,highest values turn out to be recommendations for that particular user(marked in green). \n",
        "\n",
        "\n",
        "\n",
        "* Latent factor model based collaborative filtering learns the user-item profiles( dimension K) through matrix factorization by minimizing the Root Mean Squared Error(RMSE) between the available ratings 'y' and their predicted values y^. Each item i is associated with a latent (feature) vector P, each user is associated with a latent (profile) vector U, and the rating y^(ui).\n",
        "\n",
        "* ALS uses L2 regularization to reduce the errors by fitting the function appropriately on the given training set and avoid overfitting by reducing the complexity.The weights of features are handled by L2 regularization.L2 regularization forces weights towards zero but it does not make them exactly zero as it removes a small percentage of weights after each iteration. The parameter to tune is Lambda.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1oWkNZj_Qtt4uw2vgDofJw9DV76JW6P7i)\n",
        "\n",
        "\n",
        "* Finally the way ALS works is shown in the image below:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1oH6sYxD-RX5NEt2o-eRknjwGBT49R74e)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* The values of U and P are generated by alternating the multiplications. When finding/approximaitng values for one( U or P) the other(P or U) takes up random values and is fixed.\n",
        "  - Fixing U to solve for P\n",
        "  - Fixing U to solve for U\n",
        "\n",
        "* Advantage of ALS: Don't need domain knowledge, the embeddings are automatically learnt.\n",
        "* DisAdvantage of ALS : if an item is not seen during training, the system will not be able to create an embedding for it and query the model with this item. This issue is often called the ***cold-start problem*** .\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EfUR4JvGleu1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QEYI7ETGxD2"
      },
      "source": [
        "### Calculating the Sparsity of data ###\n",
        "\n",
        "99.96% sparse(empty)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yD53CX-pDkL"
      },
      "source": [
        "# Getting Sparsity\n",
        "num = amazon_spark_df.select('overall').count()\n",
        "\n",
        "#distinct user ids and items\n",
        "user = amazon_spark_df.select('reviewerID').distinct().count()\n",
        "items = amazon_spark_df.select('asin').distinct().count()\n",
        "\n",
        "den = user * items\n",
        "\n",
        "spars = (1-(num*1)/den) * 100\n",
        "print('sparsity is ', \"%.3f\"%spars+\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0WpCoBmHzDI"
      },
      "source": [
        "### Selecting only required columns for model generation ###\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDxWE7dhwMZ7"
      },
      "source": [
        "# Getting only ratings and product id\n",
        "new_df = amazon_spark_df.select('asin','overall','reviewerID','Average_rating_by_product','Average_rating_by_user')\n",
        "new_df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGNeUJdsI7E_"
      },
      "source": [
        "### Converting all columns into numeric format for ALS ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd56e1En1V4x"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "#Converting all columns into numeric for als model using string indexer\n",
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(new_df) for column in list(set(new_df.columns))]\n",
        "\n",
        "#creating a pipeline for the model to transform it using string indexer\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "t2 = pipeline.fit(new_df).transform(new_df)\n",
        "t2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Zu7DMoHXJr"
      },
      "source": [
        "#t = t2.filter(t2.reviewerID_index == 1127)\n",
        "#t.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnvkWYk8JnE6"
      },
      "source": [
        "### Selecting only required columns from indexed(converted) dataframe ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP_xkOCeqmpy"
      },
      "source": [
        "t3 = t2.select(['asin_index', 'reviewerID_index','asin','reviewerID','overall'])\n",
        "t3.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfFOVWGKJyHi"
      },
      "source": [
        "### Splitting Dataframe into 70% training, 30% testing ###\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVS7epvdMRPh"
      },
      "source": [
        "# Splitting data from transformed :\n",
        "train, test = t3.randomSplit([0.7, 0.3], seed=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UE01_eVIIZR"
      },
      "source": [
        "train.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFF0DVJQIJyM"
      },
      "source": [
        "test.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLOcxaJQJ6dS"
      },
      "source": [
        "### Generating ALS Model and fitting with parameter tuning ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used paramter tuning with cross validation to take the best model possible.\n",
        "Values chosen for Rank = []\n",
        "Values chosen for lambda(regularization parameter) = [0.05,0.1]\n",
        "\n",
        "Did 3-fold cross validation using the training split. Because the size of data is huge and comparing more models is time consuming , we took parameters less models for comparision.\n"
      ],
      "metadata": {
        "id": "1g5URV07pWT7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTfW7MmgNSnv"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "als = ALS(maxIter=5, userCol=\"reviewerID_index\",itemCol=\"asin_index\",\n",
        "          ratingCol=\"overall\",coldStartStrategy=\"drop\",nonnegative=True)\n",
        "\n",
        "# Testing 4 models:\n",
        "param_grid = ParamGridBuilder() \\\n",
        "            .addGrid(als.rank, [10,25]) \\\n",
        "            .addGrid(als.regParam, [0.05,0.1]) \\\n",
        "            .build()\n",
        "\n",
        "# Define evaluator as RMSE and print length of evaluator\n",
        "evaluator = RegressionEvaluator(\n",
        "           metricName=\"rmse\", \n",
        "           labelCol=\"overall\", \n",
        "           predictionCol=\"prediction\") \n",
        "\n",
        "# Build cross validation using CrossValidator\n",
        "cv = CrossValidator(estimator=als, \n",
        "                    estimatorParamMaps=param_grid, \n",
        "                    evaluator=evaluator, numFolds=3)\n",
        "#Fit cross validator to the 'train' dataset\n",
        "models = cv.fit(train)\n",
        "#Extract best model from the cv model above\n",
        "model = models.bestModel\n",
        "\n",
        "#als = ALS(maxIter=5,regParam=0.09,rank=30,userCol=\"reviewerID_index\",itemCol=\"asin_index\",ratingCol=\"overall\",coldStartStrategy=\"drop\",nonnegative=True)\n",
        "#model=als.fit(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = models.bestModel\n",
        "print(\"For Best Model : \")\n",
        "print(\"Rank:\", best_model._java_obj.parent().getRank())\n",
        "print(\"RegParam:\", best_model._java_obj.parent().getRegParam())"
      ],
      "metadata": {
        "id": "Z6XUREdPwd0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLXXzY2v_1LN"
      },
      "source": [
        "### Model Evaluation (RMSE) ###\n",
        "\n",
        "Getting RMSE for the best model that was generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ma0DoSse_jeV"
      },
      "source": [
        "#model = all_models.bestModel\n",
        "\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.mllib.evaluation import RegressionMetrics\n",
        "#evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"overall\",predictionCol=\"prediction\")\n",
        "predictions = best_model.transform(test)\n",
        "RMSE =evaluator.evaluate(predictions)\n",
        "print(\"RMSE=\"+str(RMSE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xwDeon3CXQY"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL6wnHJZ6J_d"
      },
      "source": [
        "### Recommendations for Users ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* In ALS using the user similarity, we can get recommendations. We used the recommendForAllUsers fucntion that is avilable in spark.\n",
        "\n",
        "* We generated reviews for a user with index 11276.\n"
      ],
      "metadata": {
        "id": "yRzKGgTCXWTf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aokL4DBonjA_"
      },
      "source": [
        "#Recommendations for users\n",
        "from pyspark.sql.types import StructField\n",
        "from pyspark.sql.types import StructType\n",
        "from pyspark.sql.types import IntegerType\n",
        "test = best_model.recommendForAllUsers(25).filter(fn.col('reviewerID_index')== 11276).select(\"recommendations\").collect()\n",
        "item_recommends = []\n",
        "for item in test[0][0]:        \n",
        "    item_recommends.append(item.asin_index)\n",
        "    \n",
        "schema = StructType([StructField(\"asin_index\",IntegerType(),True)])\n",
        "items_fin = spark.createDataFrame(item_recommends,IntegerType()).toDF(\"asin_index\")\n",
        "items_fin.sort(fn.col('asin_index'),ascending=False).show(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvMq6g6Lsqen"
      },
      "source": [
        "t2\\\n",
        ".select(['asin','asin_index', 'reviewerID','reviewerID_index', 'overall' ,'Average_rating_by_user'])\\\n",
        ".filter(fn.col('reviewerID_index')== 25234)\\\n",
        ".sort(fn.col('asin_index'),ascending = False)\\\n",
        ".show()\n",
        "\n",
        "tab = items_fin\\\n",
        ".join(t2, on = 'asin_index', how = 'inner')\\\n",
        ".select(['asin','asin_index','reviewerID','reviewerID_index','overall','Average_rating_by_product','Average_rating_by_user'])\\\n",
        ".drop_duplicates(subset=['asin'])\\\n",
        ".sort(fn.col('asin_index'),ascending = False)\\\n",
        ".collect()\n",
        "\n",
        "tv = spark.createDataFrame(tab)\n",
        "tv.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpIOMg78mzZ2"
      },
      "source": [
        "user_df = merged_df.select('reviewerID','reviewerName')\n",
        "user_df.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tl-UtTYnjLP"
      },
      "source": [
        "#Getting recommendations for the above chosen user based on other users.\n",
        "user_df = merged_df.select('reviewerID','reviewerName')\n",
        "user_df\\\n",
        ".join(tv,on = 'reviewerID',how = 'inner')\\\n",
        ".distinct()\\\n",
        ".sort(fn.col('asin_index'),ascending =False)\\\n",
        ".show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpnPTjx0keKI"
      },
      "source": [
        "# For user :  25492.0 example\n",
        "\n",
        "test = model.recommendForAllUsers(20).filter(fn.col('reviewerID_index')== 25492).select(\"recommendations\").collect()\n",
        "item_recommends = []\n",
        "for item in test[0][0]:        \n",
        "    item_recommends.append(item.asin_index)\n",
        "    \n",
        "schema = StructType([StructField(\"asin_index\",IntegerType(),True)])\n",
        "items_fin = spark.createDataFrame(item_recommends,IntegerType()).toDF(\"asin_index\")\n",
        "items_fin.sort(fn.col('asin_index'),ascending=False).show()\n",
        "\n",
        "t2\\\n",
        ".select(['asin','asin_index', 'reviewerID','reviewerID_index', 'overall' ,'Average_rating_by_user'])\\\n",
        ".filter(fn.col('reviewerID_index')== 25492)\\\n",
        ".sort(fn.col('asin_index'),ascending = False)\\\n",
        ".show()\n",
        "\n",
        "tab = items_fin\\\n",
        ".join(t2, on = 'asin_index', how = 'inner')\\\n",
        ".select(['asin','asin_index','reviewerID','reviewerID_index','overall','Average_rating_by_product'])\\\n",
        ".drop_duplicates(subset=['asin'])\\\n",
        ".sort(fn.col('asin_index'),ascending = False)\\\n",
        ".collect()\n",
        "\n",
        "\n",
        "tv = spark.createDataFrame(tab)\n",
        "tv.show()\n",
        "\n",
        "#Getting recommendations for the above chosen user based on other users.\n",
        "user_df = merged_df.select('reviewerID','reviewerName')\n",
        "\n",
        "user_df\\\n",
        ".join(tv,on = 'reviewerID',how = 'inner')\\\n",
        ".distinct()\\\n",
        ".sort(fn.col('asin_index'),ascending =False)\\\n",
        ".show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucnr6UC6Llcf"
      },
      "source": [
        "## **4. Sentiment Analysis** ## "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview of customer sentiment analysis:\n",
        "\n",
        "Using the text review that was given by the user, sentiment analysis was performed using Logistic regression to understand the overall sentiment of the user.\n",
        "\n",
        "Used all reviews given by the user to get their sentiment score. If their overall sentiment score is negative and the average rating given by them is <=2( we decided the threshold) or if the user overall sentiment score is positive and rating = 5, then further analysis should be peformed, where you look into the products reviewed to see if they are targetting a particular company's products.\n",
        "\n",
        "* The steps that were followed are: \n",
        "\n",
        "  1. For the input data:\n",
        "\n",
        "     a) Tokenizing words( breaking down sentences into words)\n",
        "\n",
        "      b) Removing stop words (commonly occuring filler words like articles, pronouns etc)\n",
        "\n",
        "      c).Converting the words into features(The column features is a sparse vector representation of the words that appeared in the text\n",
        "  2. Get positive and negative words from Parquet file\n",
        "  3. For each review, generating the sentiment score for each review( 1 for positive and 0 for negative). Positive implies average score > 0.\n",
        "  4. Using Tf-idf, we reduce the words by this numerical statistic that is intended to reflect how important a word is to a review.\n",
        "\n",
        "    \\begin{equation}\n",
        "    \\text{tf-idf}_{ij} = f_{ij} \\log \\frac{|D|+1}{f_i+1}\n",
        "    \\end{equation}\n",
        "\n",
        "  5. Using logistic regression to classify if the given review based on the tf-idf features, is positive(1) or negative(0).\n",
        "  6. Flag users based on their sentiment score, average rating\n",
        "    - Negative/flagged if sentiment score = 0 and average rating < 2\n",
        "    - overly positive if sentiment score  = 1 and average rating = 5\n",
        "\n",
        "\n",
        "> Logistic Regression:\n",
        "\n",
        "\n",
        "  - The target variable here is to predict positive/negative sentiment. This is categorical. Binary logistic regression is used.\n",
        "\n",
        "  - It uses linear or non-linear sigmoid function as decision boundary.\n",
        "\n",
        "    \\begin{equation}\n",
        "    \\text{y} =  \\frac{1}{1+exp(x)}\n",
        "    \\end{equation}\n",
        "\n",
        "\n",
        "  - To avoid overfitting Elastic net Regularization is used, which is a combination of both L1 and L2 regularization.\n",
        "  \\begin{equation}\n",
        "L_\\theta^{\\lambda,\\alpha}(p(X),Y) = -\\left( \\sum_i Y_i \\log p_\\theta(X_i) + (1-Y_i)\\log (1-p_\\theta(X_i)) \\right) + \\lambda \\left[(1-\\alpha) \\sum_{j>0} \\theta_j^2 + \\alpha \\sum_{j>0} \\left| \\theta_j \\right| \\right]\n",
        "\\end{equation}\n",
        "\n",
        "  - HyperParamter tuning was performed, to select the model that gave the highest accuracy( meaning reduced cost). 60:30:10 training:validation:testing split was used. Based on the best accuracy generated, the final model was chosen to fit the testing set.\n",
        "\n",
        "* We have 3 phases : Using only logistic regression(positive and negative words are overfit, weights are wrongly assigned), using Logistic regression with Elastic Net Regularization( weights are corrected) and final is parameter tuning with different lambda values to get the highest accuracy."
      ],
      "metadata": {
        "id": "Qb6EuTemupV2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCvObV92I7BQ"
      },
      "source": [
        "from __future__ import division\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import feature, regression, evaluation, Pipeline\n",
        "from pyspark.sql import functions as fn, Rowl\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark.ml.param import *\n",
        "from pyspark.ml.tuning import *\n",
        "from pyspark.ml.evaluation import *\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import rand \n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.tuning import ParamGridBuilder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing, stop word removal, feature generation ###"
      ],
      "metadata": {
        "id": "OjpmaMR0sX0K"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj7Y-1rWLuRI"
      },
      "source": [
        "#String Indexer\n",
        "sentiment_df = amazon_spark_df.select('asin','reviewerID','reviewerName','reviewText','summary','Average_rating_by_user')\n",
        "sentiment_df = sentiment_df.dropna()\n",
        "indexers2 = StringIndexer(inputCol='reviewerID', outputCol = \"reviewerID_index\").fit(sentiment_df)\n",
        "indexers1 = StringIndexer(inputCol='asin', outputCol = \"asin_index\").fit(sentiment_df)\n",
        "\n",
        "#Tokenizer\n",
        "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"words\", pattern=\"\\W\")##'\\w' remove none-word letters\n",
        "df_tokenized = tokenizer.transform(sentiment_df)\n",
        "\n",
        "import requests\n",
        "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "sw_filter = StopWordsRemover()\\\n",
        "  .setStopWords(stop_words)\\\n",
        "  .setCaseSensitive(False)\\\n",
        "  .setInputCol(\"words\")\\\n",
        "  .setOutputCol(\"filtered\")\n",
        "\n",
        "#count vectorizer :\n",
        "count_vectorizer_estimator = CountVectorizer().setInputCol('words').setOutputCol('features')\n",
        "count_vectorizer_transformer = count_vectorizer_estimator.fit(df_tokenized)\n",
        "count_vectorizer_transformer.transform(df_tokenized)\n",
        "\n",
        "pipeline_sent = Pipeline(stages=[indexers1,indexers2,tokenizer,sw_filter,count_vectorizer_estimator]).fit(sentiment_df)\n",
        "t2_sent = pipeline_sent.transform(sentiment_df)\n",
        "t2_sent.show()\n",
        "t2_sent = t2_sent.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wIOrOjlB6oW"
      },
      "source": [
        "t2_sent.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sentiments parquet file ###"
      ],
      "metadata": {
        "id": "G55t2r5ardNQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOEgCZxvF6Hc"
      },
      "source": [
        "sentiments_df = spark.read.parquet('/content/drive/MyDrive/sentiments.parquet')\n",
        "sentiments_df.printSchema()\n",
        "sentiments_df.where(fn.col('sentiment') == 1).show(5)\n",
        "sentiments_df.where(fn.col('sentiment') == -1).show(5)\n",
        "sentiments_df.groupBy('sentiment').agg(fn.count('*')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating sentiment value for each word\n"
      ],
      "metadata": {
        "id": "R3GRmq-QroMh"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RVCeCdnuNQX"
      },
      "source": [
        "t2_split = t2_sent.select('*',fn.explode('words').alias('word')).join(sentiments_df,'word')\n",
        "t2_split.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating average sentiment based on sentiment score ###"
      ],
      "metadata": {
        "id": "mkLsk2YzrNvZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwVkNiocExr9"
      },
      "source": [
        "t2_split_sentiment_prediction = t2_split.\\\n",
        "    groupBy('asin', 'reviewerID').\\\n",
        "    agg(fn.avg('sentiment').alias('avg_sentiment')).\\\n",
        "    withColumn('score', fn.when(fn.col('avg_sentiment') > 0, 1.0).otherwise(0.))\n",
        "t2_split_sentiment_prediction.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuSgHoGfw6qh"
      },
      "source": [
        "sentiment_score_df = t2_split.select('*')\\\n",
        ".join(t2_split_sentiment_prediction,on = ['asin','reviewerID'],how = 'inner')\n",
        "\n",
        "sentiment_score_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Flag column: ###\n",
        "##### rating < 2,sentiment = 0, \"F\" #####\n",
        "#####  rating = 5 , sentiment = 1,\"P\" #####\n",
        "##### other cases, NA #####"
      ],
      "metadata": {
        "id": "LjCXZrlvoeR0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPUDQxGGahBn"
      },
      "source": [
        "sentiment_score = sentiment_score_df.select(['asin', 'reviewerID', 'reviewerName', 'reviewText', 'summary','Average_rating_by_user' ,'sentiment', 'avg_sentiment', 'score']).\\\n",
        "                    distinct()\n",
        "\n",
        "\n",
        "sentiment_score = sentiment_score.withColumn('flag',\n",
        "                                             when((fn.col('Average_rating_by_user') < 2 ) & (fn.col('score') == 0 ),'F')\\\n",
        "                                             .when((fn.col('Average_rating_by_user') == 5 ) & (fn.col('score') == 1 ),'P').otherwise('NA'))\n",
        "sentiment_score.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF ###"
      ],
      "metadata": {
        "id": "yyocDD_cpR-D"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6RjNctLiSvB"
      },
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "idf = IDF().\\\n",
        "    setInputCol('features').\\\n",
        "    setOutputCol('tfidf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgLEIVpwoPTT"
      },
      "source": [
        "idf_pipeline = Pipeline(stages=[pipeline_sent,idf]).fit(sentiment_score)\n",
        "idf_pipeline.transform(sentiment_score).show(20)\n",
        "tfidf_df = idf_pipeline.transform(sentiment_score)\n",
        "#tfidf_df = idf_pipeline.transform(sentiment_score_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train ,Validation, Test split ###"
      ],
      "metadata": {
        "id": "vfPnwsLypYpU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ1lwXnWopmD"
      },
      "source": [
        "training_df, validation_df, testing_df = sentiment_score.randomSplit([0.6, 0.3, 0.1], seed=0)\n",
        "[training_df.count(), validation_df.count(), testing_df.count()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression ( simple) ###"
      ],
      "metadata": {
        "id": "WkyJbcpCpfTn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd-J2gsho2Lc"
      },
      "source": [
        "lr = LogisticRegression().\\\n",
        "    setLabelCol('score').\\\n",
        "    setFeaturesCol('tfidf').\\\n",
        "    setRegParam(0.0).\\\n",
        "    setMaxIter(100).\\\n",
        "    setElasticNetParam(0.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK-UCg7Lo6Wk"
      },
      "source": [
        "lr_pipeline = Pipeline(stages=[idf_pipeline, lr]).fit(training_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amjtKOWto9Dp"
      },
      "source": [
        "lr_pipeline.transform(validation_df).\\\n",
        "    select(fn.expr('float(prediction = score)').alias('correct')).\\\n",
        "    select(fn.avg('correct')).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nsz4WQqpEjM"
      },
      "source": [
        "vocabulary = idf_pipeline.stages[0].stages[-1].vocabulary\n",
        "weights = lr_pipeline.stages[-1].coefficients.toArray()\n",
        "coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': weights})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfpAM0uZpTqd"
      },
      "source": [
        "coeffs_df.sort_values('weight').head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sllMfcXAZ0J2"
      },
      "source": [
        "coeffs_df.sort_values('weight', ascending=False).head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression( with Elastic net Regularization) ###"
      ],
      "metadata": {
        "id": "2xpSChK3plpo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "741oSr2R7YUw"
      },
      "source": [
        "lambda_par = 0.02\n",
        "alpha_par = 0.3\n",
        "en_lr = LogisticRegression().\\\n",
        "        setLabelCol('score').\\\n",
        "        setFeaturesCol('tfidf').\\\n",
        "        setRegParam(lambda_par).\\\n",
        "        setMaxIter(100).\\\n",
        "        setElasticNetParam(alpha_par)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QB5jYK6t5Uv"
      },
      "source": [
        "en_lr_estimator = Pipeline(\n",
        "    stages=[tokenizer, sw_filter, count_vectorizer_estimator, idf, en_lr])\n",
        "\n",
        "en_lr_pipeline = en_lr_estimator.fit(training_df)\n",
        "en_lr_pipeline.transform(validation_df).select(fn.avg(fn.expr('float(prediction = score)'))).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey5DcaNKuFQJ"
      },
      "source": [
        "en_weights = en_lr_pipeline.stages[-1].coefficients.toArray()\n",
        "en_coeffs_df = pd.DataFrame({'word': en_lr_pipeline.stages[2].vocabulary, 'weight': en_weights})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKi-4VXsuLa6"
      },
      "source": [
        "en_coeffs_df.sort_values('weight').head(15)\n",
        "en_coeffs_df.sort_values('weight', ascending=False).head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6coRkqXijoFX"
      },
      "source": [
        "en_coeffs_df.sort_values('weight').head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdzLkVifusYs"
      },
      "source": [
        "en_coeffs_df.query('weight == 0.0') # There are words that have 0 weight."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression with Parameter tuning ###"
      ],
      "metadata": {
        "id": "gOQuZjDEpu0P"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCvFRaxV01H9"
      },
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "grid = ParamGridBuilder().\\\n",
        "    addGrid(en_lr.regParam, [0., 0.01, 0.02]).\\\n",
        "    addGrid(en_lr.elasticNetParam, [0., 0.2, 0.4]).\\\n",
        "    build()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfzO3g1k01gT"
      },
      "source": [
        "all_models = []\n",
        "for j in range(len(grid)):\n",
        "    print(\"Fitting model {}\".format(j+1))\n",
        "    model = en_lr_estimator.fit(training_df, grid[j])\n",
        "    all_models.append(model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUlLBlQ9012A"
      },
      "source": [
        "# estimate the accuracy of each of the models:\n",
        "accuracies = [m.\\\n",
        "    transform(validation_df).\\\n",
        "    select(fn.avg(fn.expr('float(score = prediction)')).alias('accuracy')).\\\n",
        "    first().\\\n",
        "    accuracy for m in all_models]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy7_At2dPBLH"
      },
      "source": [
        "accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltV24ydZvYBK"
      },
      "source": [
        "best_model_idx = np.argmax(accuracies)\n",
        "best_model_sent = all_models[best_model_idx]\n",
        "accuracies[best_model_idx]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Best Model in sentiment: \")\n",
        "print(\"Rank:\", best_model_sent._java_obj.parent().getRank())\n",
        "print(\"RegParam:\", best_model_sent._java_obj.parent().getRegParam())"
      ],
      "metadata": {
        "id": "0V6I_yKswVAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing using best model ###"
      ],
      "metadata": {
        "id": "3l5EPwe1p97h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# estimate performance\n",
        "best_model_sent.\\\n",
        "    transform(testing_df).\\\n",
        "    select(fn.avg(fn.expr('float(score = prediction)')).alias('accuracy')).\\\n",
        "    show()"
      ],
      "metadata": {
        "id": "T0ApPsC-p85B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}